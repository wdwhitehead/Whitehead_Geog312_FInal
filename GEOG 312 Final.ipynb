{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b11cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install leafmap\n",
    "!{sys.executable} -m pip install geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf6f594",
   "metadata": {},
   "source": [
    "1. Data Manipulation (5 points): Write a Python function that takes a list of lists representing points (latitude, longitude) and returns the average latitude and average longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cccfd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "xylist1 = [84.62, 63.11] #lists are formatted (lat,long)\n",
    "xylist2 = [89.88, 65.89]\n",
    "xylist3 = [78.93, 71.23]\n",
    "##These lists are only an example, they could be data drawn from a file and formatted into lists\n",
    "\n",
    "xymaster = [xylist1, xylist2, xylist3] #same here, if drawing from a file this list would have to automate the inputs rather tha just listing names\n",
    "lat_sum = 0\n",
    "long_sum = 0\n",
    "\n",
    "for point in xymaster:\n",
    "  lat_sum += point[0]\n",
    "lat_avg = lat_sum / len(xymaster)\n",
    "\n",
    "for point in xymaster:\n",
    "  long_sum += point[1]\n",
    "long_avg = long_sum / len(xymaster)\n",
    "\n",
    "print(f'Average Latitude: {lat_avg}\\nAverage Longitude: {long_avg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b6b4f",
   "metadata": {},
   "source": [
    "2. Conditional Statements (5 points): Write a Python program that reads a CSV file containing elevation data for points. The program should then identify and print the points with an elevation above a user-specified threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614afd68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import leafmap\n",
    "\n",
    "elev_csv = 'https://raw.githubusercontent.com/wdwhitehead/Whitehead_Geog312_Final/main/elevation.csv'\n",
    "df = leafmap.csv_to_df(elev_csv)\n",
    "index = 0\n",
    "message = 'Pick an elevation threshold. Points above the threshold will be displayed.'\n",
    "thold = input(message)\n",
    "thold = int(thold)\n",
    "try:\n",
    "    for point in elev_csv:\n",
    "        elev = df['elevation'].loc[df.index[index]]\n",
    "        if elev >= thold:\n",
    "            print(f'{elev}')\n",
    "        index += 1\n",
    "except IndexError: #this is dumb but I didn't know how to stop it from giving this error so I'm working around it\n",
    "        print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddac6995",
   "metadata": {},
   "source": [
    "3. Loops (5 points): Write a Python program that iterates through a dictionary containing countynames and their corresponding population densities. The program should calculate and print the total population for all counties. (Hint: Use a random number generator or google the population of your chosen cities to create the values for the population density. Use 10 different city names.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f28996",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "density_dict = { #pop data from tennessee-demographics.com, values are pop over sq miles\n",
    "    'Shelby': 1200.742953732458,\n",
    "    'Davidson': 1404.9639902386764,\n",
    "    'Knox': 973.1685720469885,\n",
    "    'Hamilton': 690.7471931862176,\n",
    "    'Rutherford': 582.244575045208,\n",
    "    'Williamson': 447.67421901819426,\n",
    "    'Montgomery': 437.43833228235474,\n",
    "    'Sumner': 385.03730286146,\n",
    "    'Sullivan': 389.055544803561,\n",
    "    'Wilson': 277.7622059106914\n",
    "}\n",
    "area_dict = { #In sq miles, data from usa.com\n",
    "    'Shelby': 763.17,\n",
    "    'Davidson': 504.03,\n",
    "    'Knox': 508.21,\n",
    "    'Hamilton': 542.43,\n",
    "    'Rutherford': 619.36,\n",
    "    'Williamson': 582.60,\n",
    "    'Montgomery': 539.18,\n",
    "    'Sumner': 529.45,\n",
    "    'Sullivan': 413.36,\n",
    "    'Wilson': 570.83\n",
    "}\n",
    "index = 0\n",
    "for key, value in density_dict.items():\n",
    "  population = value * (area_dict[key])\n",
    "  population = int(population)\n",
    "  index += 1\n",
    "  print(f'{key} County Population: {population}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bec811",
   "metadata": {},
   "source": [
    "4. Functions with Arguments (5 points): Write a Python function that takes a shapefile path as input and returns its area in square kilometers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c23a849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import leafmap\n",
    "\n",
    "message = \"Please enter your shapefile's filepath:   \" #paste \"https://github.com/giswqs/leafmap/raw/master/examples/data/countries.zip\"\n",
    "in_shp = input(message)\n",
    "\n",
    "m = leafmap.Map(center=[0, 0], zoom=2)\n",
    "m.add_vector(in_shp, layer_name=\"shapefile\")\n",
    "leafmap.vector_area(in_shp, 'km2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d160d97",
   "metadata": {},
   "source": [
    "5. Error Handling (5 points): Write a Python program that attempts to read a raster file. If the file is not found or invalid, the program should print a helpful error message and gracefully exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I didn't have this program actually do anything with the file, just read it.\n",
    "#I also know that .tif isn't the only raster file extension, but this would be insanely long if I included every single one, so I assume that is not the intention.\n",
    "raster_file = '/content/TN_Knoxville_147959_1978_24000_geo.tif'\n",
    "if raster_file.endswith('.tif') == True: #this is a super convenient method. It's neat that it exists\n",
    "  try:\n",
    "    with open(raster_file) as file_object:\n",
    "      contents = file_object.read()\n",
    "  except FileNotFoundError:\n",
    "    print(f'{raster_file} not found.')\n",
    "else:\n",
    "  print(f'{raster_file} is invalid. Please input a .tif file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b07cd19",
   "metadata": {},
   "source": [
    "1. Problem-Solving (10 points): You are tasked with creating a program to identify areas suitable for building a new solar farm. You have access to datasets for land cover, slope, and solar radiation. Describe the workflow of your program, including data preparation, analysis steps, and final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c8d36",
   "metadata": {},
   "source": [
    "Primarily, it will be useful to identify 3 things:\n",
    "1.) A large area with low to zero slope\n",
    "2.) An area with high solar radiation\n",
    "3.) An area that will take low amounts of pre-construction preparation, such as a clearing rather than a forest (but this is a lower priority)\n",
    "\n",
    "\n",
    "Data prep:\n",
    "\n",
    "We can first work with our slope raster to only select raster cells that have a slope of 1.0 degrees or less. We will reclass all cells that have 0-1.0 degrees slope as \"1\", and everything else as \"0\" using the \"reclass\" tool from Whitebox.\n",
    "\n",
    "Then, we will perform the same workflow, only using the solar radiation raster. I don't really know what acceptable solar radiation would be, but whatever that value is, anything greater than or equal to it will be assigned as \"1\".\n",
    "\n",
    "For our last raster, we will take the land cover raster and reclass plains, farmland, and other clear areas as \"2\". Water and other types of land cover that cannot be built on get reclassed to \"0\". Everything else gets \"1\".\n",
    "\n",
    "\n",
    "Analysis:\n",
    "\n",
    "We will use whitebox's raster_calculator to multiply our 0/1 solar raster, our 0/1 slope raster, and our 0/1/2 land cover raster. We will be left with a raster covered in values of 0, 1, and 2.\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "Our resulting 0/1/2 raster contains the suitability of each cell. 0 is not suitable for solar construction. 1 is suitable, but not ideal. 2 is ideal. Visual examination will provide large areas of 2 and 1 cells, these areas will be most suitable for solar farms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03bdb05",
   "metadata": {},
   "source": [
    "2. Critical Thinking (10 points): You are given a shapefile of earthquake epicenters and a raster file of population density. Explain how you would use Python and GIS libraries to identify the areas most vulnerable to earthquake damage, considering both population density and proximity to epicenters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cc0f5d",
   "metadata": {},
   "source": [
    "Using the buffer tool in whitebox, you could create a buffer of some unspecified length for each epicenter.\n",
    "\n",
    "Let's assume we have some sort of volatility field or something similar for each point. You could have a scaling distance on the buffer based on how volatile the epicenter is. (if we don't have that data, you could just have each buffer be 10 miles or something. I really don't know enough about earthquakes to decide that, though.)\n",
    "\n",
    "You could then do zonal statistics on the raster cells inside the buffer. Perhaps take the average value of population density for each epicenter, and then apply that value to a new field on the shapefile for each point.\n",
    "\n",
    "We could then display each point scaling with color and/or size based on that average population density value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd23163",
   "metadata": {},
   "source": [
    "3. Critical Thinking (10 points): You are tasked with creating a visualization that shows the impacts of climate change on Tennessee agricultural production. You are given a dataset for precipitation, temperature, and crop output. How would you go about visualizing this in python and what analyses would need to be done to measure these impacts?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040d87d",
   "metadata": {},
   "source": [
    "This would be a great use for the geemap timelapse feature. I am not confident how easy it is to import your own datasets rather than using the imagecollections already specified in the tool, however.\n",
    "\n",
    "Using your timelapse, you could display all three maps simultaneously, allowing users to examine each timelapse on their own.\n",
    "\n",
    "One way you could create some additional analysis to add to a table next to the visualization would be to create yearly  imagecollections with your available datasets to perform descriptive statistics on the collection. You could show how each year average values change. This is rather dependent on the data in hand, however. If you only have 1 image per year already including averaged values, you wouldn't need an imagecollection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33141fe",
   "metadata": {},
   "source": [
    "4. Geospatial Analysis (10 points): You have a shapefile of national parks and a raster file of deforestation rates. Design a Python program to calculate the total area of deforestation within each national park and identify the park with the highest deforestation rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87c1064",
   "metadata": {},
   "source": [
    "One way to calculate the total area deforested is by using the zonal statistics by group tool to take a percentage of the area of each polygon covered by different values in the raster. Taking these numbers, we can see which polygons have the highest percentage of coverage by deforestation. With this list of values, we can use MAX to determine the highest percent coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db28ecb3",
   "metadata": {},
   "source": [
    "1.Shapefile Visualization (10 points): Write a Python program to read a shapefile. Create a map\n",
    "showing the data points as colored polygons (anything other than circles). Each data point should\n",
    "be colored based on its magnitude (e.g., green for small (low), yellow for medium (average), red\n",
    "for large (high)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f192a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopandas\n",
    "!pip install shapely "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9196b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "\n",
    "#I cannot figure out the way to get my shapefiles to run straight off of GitHub. \n",
    "#If you download the \"pointmag.shp\" file off of my repository and fix the path, this code block works perfectly.\n",
    "shapefile_path = '/content/pointmag.shp' \n",
    "\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "gdf.plot(column=\"Magnitude\", ax=ax, cmap='RdYlGn', edgecolor='k',marker='D')\n",
    "\n",
    "ax.set_title('Magnitude Points in Tennessee')\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34013e6",
   "metadata": {},
   "source": [
    "2. Raster Processing and Visualization (10 points): Write a Python program to read a raster file of\n",
    "precipitation data and apply a colormap to visualize the temperature variations. Add a legend to\n",
    "the map and ensure clear labels for axes and title.\n",
    "*Hint: Use the precipitation dataset already provided to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c72420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I cannot find the precipitation dataset anywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1409769b",
   "metadata": {},
   "source": [
    "3. Cartographic Design (10 points): Design a map showing the distribution of solar farms in\n",
    "west, middle, and east Tennessee. Create a raster layer that summarizes the total energy\n",
    "generated from the solar farms in each of these regions (it can be one raster layer). Discuss which\n",
    "region has the most solar production and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301dffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I spent 2 hours trying to figure this out and I do not know how I should make that raster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc20488",
   "metadata": {},
   "source": [
    "Bonus Question (10 points):\n",
    "Develop a Python program that automates a repetitive GIS task you encountered during your\n",
    "project or coursework. Explain the task, your approach, and the benefits of automation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1d0c1c",
   "metadata": {},
   "source": [
    "One task I do all the time at my job is assigning points identification numbers in a new field based on the category in a different field.\n",
    "\n",
    "It's under NDA, but I can obfuscate in my example. \"category\" is my input, and \"new field\" is what my output is supposed to look like.\n",
    "\n",
    "category : Red11\n",
    "new field: 11\n",
    "\n",
    "\"new field\" has to be an integer, not a string, so we can do math operations. Normally I do this manually for each point in the shapefile. It is really boring, takes probably 2-3 hours a week, and I've been thinking about optimizing it. Python would be perfect.\n",
    "\n",
    "\n",
    "I can use python3 code in the calculate field function of ArcGIS pro to cut off the end of the string in the category field by selecting the final two characters, and then classify that variable as an integer. I can then put that variable in the \"calculate field\" section, and it will iterate through each item in the list, doing the work automatically. \n",
    "\n",
    "I'm actually going to do this tomorrow at my job. This will save me a bunch of time in the future, and I'll be able to pass off the script to my coworkers as well. Between the 3 of us, it could save 6-10 hours per week."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
